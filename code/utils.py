import itertools
import pickle
import random
import sys

import zmq

import sentient.csv_pb2 as csv_pb2


def file_to_wordset(filename):
    ''' Converts a file with a word per line to a Python set '''
    words = []
    with open(filename, 'r') as f:
        for line in f:
            words.append(line.strip())
    return set(words)


def write_status(i, total):
    ''' Writes status of a process to console '''
    sys.stdout.write('\r')
    sys.stdout.write('Processing %d/%d' % (i, total))
    sys.stdout.flush()


def save_results_to_csv(results, csv_file):
    ''' Save list of type [(tweet_id, positive)] to csv in Kaggle format '''
    with open(csv_file, 'w') as csv:
        csv.write('id,prediction\n')
        for tweet_id, pred in results:
            csv.write(tweet_id)
            csv.write(',')
            csv.write(str(pred))
            csv.write('\n')


def top_n_words(pkl_file_name, N, shift=0):
    """
    Returns a dictionary of form {word:rank} of top N words from a pickle
    file which has a nltk FreqDist object generated by stats.py

    Args:
        pkl_file_name (str): Name of pickle file
        N (int): The number of words to get
        shift: amount to shift the rank from 0.
    Returns:
        dict: Of form {word:rank}
    """
    with open(pkl_file_name, 'rb') as pkl_file:
        freq_dist = pickle.load(pkl_file)
    most_common = freq_dist.most_common(N)
    words = {p[0]: i + shift for i, p in enumerate(most_common)}
    return words


def top_n_bigrams(pkl_file_name, N, shift=0):
    """
    Returns a dictionary of form {bigram:rank} of top N bigrams from a pickle
    file which has a Counter object generated by stats.py

    Args:
        pkl_file_name (str): Name of pickle file
        N (int): The number of bigrams to get
        shift: amount to shift the rank from 0.
    Returns:
        dict: Of form {bigram:rank}
    """
    with open(pkl_file_name, 'rb') as pkl_file:
        freq_dist = pickle.load(pkl_file)
    most_common = freq_dist.most_common(N)
    bigrams = {p[0]: i for i, p in enumerate(most_common)}
    return bigrams


def split_data(tweets, validation_split=0.1):
    """Split the data into training and validation sets

    Args:
        tweets (list): list of tuples
        validation_split (float, optional): validation split %

    Returns:
        (list, list): training-set, validation-set
    """
    index = int((1 - validation_split) * len(tweets))
    random.shuffle(tweets)
    return tweets[:index], tweets[index:]


def parse_csv(csv_path, service_port):
    context = zmq.Context()
    service_socket = context.socket(zmq.REQ)
    service_socket.connect(f"tcp://127.0.0.1:{service_port}")
    output_socket = context.socket(zmq.PUSH)
    output_port = output_socket.bind_to_random_port("tcp://127.0.0.1")
    request = csv_pb2.ParseCsvRequest()
    request.csvPath = csv_path
    request.outputPort = output_port
    request = request.SerializeToString()
    service_socket.send(request, 0)
    response = service_socket.recv(0)
    response = csv_pb2.ParseCsvResponse.FromString(response)
    input_port = response.outputPort
    input_socket = context.socket(zmq.PULL)
    input_socket.connect(f"tcp://127.0.0.1:{input_port}")
    print(f"Parsing CSV [{csv_path}] on port [{input_port}] ...")
    while True:
        message = input_socket.recv(0)
        if len(message) == 0:
            break
        yield csv_pb2.Row.FromString(message)
    output_socket.send(b'')
    input_socket.close()
    output_socket.close()
    service_socket.close()
